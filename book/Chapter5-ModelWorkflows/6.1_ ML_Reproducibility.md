# 6.1 ML Reproducibility

**Reproducibility** of research and in particularly machine learning geoscience is a upmost challenge that scientists will be tackling in the next decade. It is important to instill trust in scientific results.

While definitions vary among research fields, we adopt the definition of the NAS report {cite:p}`engineering2019confidence`:

_Reproducibility_ and _Replicability_ are related to _Verification_ (correctness of the code) and _Validation_ (the model can predict other data).


_Reproducibility_ is obtaining consistent results using the same input
data; computational steps, methods, and code; and conditions of analysis.
This definition is synonymous with "computational reproducibility." Reproducibility involves the original data and code. The report recommends scientists to provide the following:
    - **Data**: Input data (data files, or scripts to download/generate the data) and some intermediate results if necessary.
    - **Method**: methods, computational steps, parameters, in executable form when possible
    - **Computational environment**: operating system, hardware architecture, library dependencies.
Reproducibility of the results may be accepted within a defined range of variability. Because of the complexity in machine-learning research (e.g., in the training process), deciding on acceptable range of such variation is still an active area of research.

Computational bugs can be reproduced accurately, therefore exact reproducibility (or verification) does not guarantee correctness. Validation can mitigate this.

_Replicability_ is obtaining consistent results across studies aimed at
answering the same scientific question, each of which has obtained its own
data. Two studies may be considered to have replicated if they obtain consistent results given the level of uncertainty inherent in the system under study. Replicability involves new data collection to test for consistency with previous results of a similar study.



